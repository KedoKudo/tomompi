__________________________________________________________________
TomoMPI_SinogramServer.cpp

Controlling code for TomoMPI cluster.
Developed and maintained by:
       Brian Tieman
       Argonne National Laboratory
       tieman@aps.anl.gov

8/20/2003   V1.0   BT  First version with acknowledgements
9/4/2003    V1.0   BT  Server now generates integer sinograms
        to send to the clients.  The clients then perform the
        normalization.
9/10/2003   V1.2   BT  Server now determines whether or not to autocenter
        based on a flag set in the experiment file.
1/30/2004   V1.3   BT  Upgraded cluster to run on RedHat 9.0.  This version fails
        to run properly when linked with the MKL.  A few memory issues were fixed
        as a result of this upgrade.
2/26/2004   V1.4   BT  Dark fields are now averaged before being sent to the client.
        This allows for more flexibility in the acquisition--for example, it's now
        possible to take a number of darks at the end or take darks at regular
        intervals throughout the dat set and the same reconstruction code can
        handle it transparently.
2/26/2004   V1.4   BT  Started adding ERROR lines to the log files.  This will
        hopefully make it easier to track down problems in the future. Grep all the
        log files for "ERROR" to find the error lines.  All error lines should
        reference the routine where they occured.
3/15/2004   V1.5   BT  Fixed a few problems with the darkfield average.  One was that
        there were still a few places where dark_field_sino_ave was being interpreted as
        unsigned short instead of float.  This may have been causing some problems with
        data overruns.  Also, the wrong dark_fiel_sino_ave was being forwarded to the clients.
3/15/2004   V1.5   BT  The client now recieves the centering related fields--
        fixed_shift and fixed_shift_value.  If fixed_shift is non-zero, the auto
        findcenter routine will be ignored and the value in fixed_shift_value will
        be used for the shift value for all sinograms.  For the moment, these values
        are read from reconstruction.cfg as the fields <Fixed Shift> and <Fixed Shift Value>
3/23/2004   V1.5   BT  The array used to store the average dark field was not being
        initialized prior to calculating a new average dark field.  This caused problems
        as the dark field average is calculated progressively.
5/20/2004   V1.5   BT  There is now support for intermediate calculation debug info.
        There are several points in the calculation chain where sinograms may be
        written out to disk.  The debug type is specified in the recdonstruction.cfg
        file by dsetting the <Debug> field to one of the following types:
        None -- no debuging
        SinoPreNorm -- sinograms before normalization
        SinoPostNorm -- sinograms post normlalization
        PostCentering -- sinograms after centering
        PostRingRemoval -- sinograms post ring removal
6/1/2004    V1.5  BT  Added a new debug mode
        MarkSuspicious -- look for suspicious reconstructions and note them in the
        log.  Also, try and output the whitefield, darkfield, raw sinogram, normalized
        sinogram, post centering sinogram, and post ring removal sinogram for that slice.
        This mode chews up a lot of memory so it might be better to run just a single
        client on each node.
6/7/2004    V1.6  BT  Added another new debug mode
        I broke down and did the accounting that needed to be done to write all intermediate
        results from outside the reconstruction thread.  This means that all intermediate
        results may now be written at once while avoiding thread issues with HDF not being
        thread safe.  The new debug mode is:
        Full -- write all intermediate results.  Note, this mode creates additional client
        memory and uses a ton of disk space!
6/7/2004    V1.6  BT  The new debug mode allowed me to very quickly find a fairly serious
        bug.  The dark fields were not being handled properly.  Only the first row of dark
        field was ever being handed out.  In addition, the buffer the dark field average
        was being performed into was not being adaquately protected from multi-thread access.
        In some cases, teh dark fields handed out may have been very wrong due to averages
        either not being complete, or the averaging buffer being set back to 0 prior to reading
        new buffers.  The result is that most sinograms were getting the wrong dark field 
        information with occasional sinograms recieving dark fields with very wrong values.
        Thi issue has now been resolved.
12/10/2004	V1.8  BT Built tomompi on development cluster (AMD opteron machines).  Turned on
		compiler optimizations.  Also built with MPE profiling libraries so I could start
		trying to use jumpshot to profile our performance.
12/10/2004	V1.8  BT Created a recon info record structure that contains all the initialization
		parameters needed by the clients.  This allowed for a significant reduction of MPI
		sends to the clients upon initialization as nearly the entire structure can be sent
		at once--as opposed to each datum being sent seperately.  Also did this for the sinogram
		data that was being sent each time a sinogram was requested.  This appears to have about
		a 10% impact on performance.
1/24/2006	V1.9  BT Added a reconstruction.cfg flag <Ring Removal> which can be 0 or 1.  1 means
		use ring removal.  0 means do not use ring removal.
1/31/2006	V1.10 BT Added a reconstruction.cfg parameter <Ring Removal Coeff> which can be 0= < coeff <= 5.
		This adjusts a coefficient used in the ring removal code.  See the CenteringCalss comments for
		more specific information about what this does.
3/1/2006	V1.10 BT Added a feature where random slices can be entered into a file called slices.list
		If you set the parameter <Use Slices File> in reconstruction.cfg to 1, the cluster will only
		reconstruct the files listed in slices.list.  Setting <Use Slices File> to 0 will cause the
		cluster to perform a full reconstruction as normal.
3/1/2006	V1.10 BT Fixed a bug where the first row of a sinogram was actually the wrong row--it was
		the last row of the previous sinogram.  It turns out that we should generally ignore this last
		row as it is redundant data.  It's the 180 degree angle from the first row.  The code was
		modified to ignore the last file (ie sinograms are 720 pixels high rather than 721 pixels.
3/23/2006	V1.11 BT Added a methode to compress reconstructed files.  Just using the built in loss-less
		compression in HDF provides poor compression results on the floating point reconstructed values.
		However, as the reconstruction only occupies a circle in the image, it's possible to set the corners
		to 0.0 without loosing any data.  All these 0.0 values then compress rather well yielding an
		approximate 25% reduction in file size for the saved reconstruction files.
		The appropriate reconstruction.cfg flag is <Copression Type> and its values may be:
			NONE -- no compression (corners will not even be set to 0.0
			LZW -- gzip type compression (this appears to be the best for most data sets
			RLE -- Run Length Encoding
			HUF -- Skipping-Huffman
3/23/2006	V1.11 BT While debugging the problem noted in the centeringclass.cpp on 3/22/2006 it
		was discovered that averaging the white fields can sometimes give a far superior result.
		A flag was added that allows the user to switch from using a single white field per projection
		to averaging all the white fields and using the average white field for all projections.  The
		flag is <Average White Fields> and it should be set to 1 to average all the whites or to 0
		to use a single white field (last acquired white field before acquisition of projection)
		per projection
5/12/2006   V1.12 BT Fixed a bug that cropped up with performing reconstructions with projections
		whose Y dimension is not equal to a multiple of files_per_pass.  If the projections were
		not an even multiple of files_per_pass, only the full passes were being done, the "extra"
		files that did not fill up a full pass were not being reconstructed.  It should now be possible
		to reconstruct samples of _ANY_ size.
4/23/2006	V1.13 BT Added a new debug mode:
		No Reconstruction -- perform all the steps of a reconstruction, but, to save time, do not
		actually reconstruct the data.
4/23/2006	V1.13 BT Removed MarkSuspicious from the possible debug modes--it wasn't useful.
4/23/2006	V1.13 BT  Modified handling of filter type to be done by int rather than by string.
6/15/2006	V1.13 BT  Modified the way parameters are passed into the reconstruction code as follows:
		reconstruction.cfg -- ths file only contains the data top directory, reconstruction save directory,
		and the experiment file to use.  All other configuration parameters were moved into the exp_file
		where they belong.
		override_exp_file.config -- if this file exists, it is read in _AFTER_ the exp_file is read.
		Parameters listed in the override_exp_file.config will over ride those found in the experiment
		file.
		Parameters used for the reconstruction--whether read in from the exp_file or from the
		override file--are now rewritten to the experiment file plus a <sample name>.config file that
		is saved in the same directory as the experiment file.  The intent is that the exp_file is
		always up to dat with what was done last and there is a file that can be copied to the
		override file as a simple way to batch a number of samples with the same processing.
1/10/2007   V2.0 BT Change of pace--a new major version number!  This version is in subversion control
		at svn+ssh://m81-cluster.aps.anl.gov/home/svn/repository/cpp.  The other major change is the
		upgrade to Nexus V3.0.0.  This brings with it the ability to write HDF5 files.  HDF5 will be the
		default file format and will have the extension .h5--hdf4 will retain the .hdf extension.  It
		is possible to use the override file to force HDF4.  When saving in HDF5, the only comrpession
		format available is NX_COMP_LZW--all other compression formats are not supported.  As an aside
		this is the first version built completely under Eclipse.
4/12/2007   V2.1 BT Added calculations to the clients to compute the data min and max for the entire
		reconstruction.  The clients track the min/max for all the slices they calculate then report
		their local min/max to the server who then computes the full reconstruction min/max.  The
		results are then stored in the exp file at ;experiment;reconstruction;data_range_min and
		;experiment;reconstruction;data_range_max.
4/13/2007	V2.1 BT fixed a typo on the reading of ring removal coefficient.  The code was doing atoi
		when it should have been doing atof.
4/12/2007	V2.1 BT Added ability to scale data into short ints by using a user supplied data_range_min
		and data_range_max values.  These values must be supplied prior to reconstructing as the full
		data range of the reconstruction can not be calculated until completion.  The data_range_min/
		data_range_max values of a previous reconstruction are generally good values to use.  The actual
		data_range_min/max values are stored in the exp file as ;experiment;reconstruction;scaled_data_min
		and ;experiment;reconstruction;scaled_data_max for reference.
5/17/2007   V2.2 BT Fixed some types where the Sinogram Pre-Norm and Post-Norm debug types did not work.
6/05/2007   V2.2 BT Modified the way sinograms were being read in CreateSinograms to be more efficient.
        This is still nowhere near as efficient as reading a large group of sequential lines at once.
6/05/2007   V2.2 BT Fixed a problem with memory allocation for the sinogram buffers.  Turns out the
        buffers were being allocated for sinograms with 1 line less than needed (eg: 719 instead of 720)
        In general, this did not seem to cause problems, but when using the slice file, the system
        would segment fault or cause some other kind of memory error when freeing the sinogram buffer
        memory.
6/22/2007   V2.2 BT Fixed a problem with the reconstruction file list files not having the correct
		extension when HDF5 was the selected output file type.
6/22/2007	V2.2 BT Fixed a problem where the total reconstruction time was not being put into the
		experiment file correctly.
11/14/2007	V2.3 BT Added a BIN file type to the output file types.  If a user selects BIN as the
		<output file type> the output files will be written as binary without any sort of header.
		Eventually, we will need a bit of a header, but we haven't settled on one yet.  Bin files
		can not be compressed
11/14/2007	V2.3 BT Made some hacks to test the performance of reading binary projection files
		vs reading HDF projection files.  The results are phenomonal!  The sinogram creation time
		plummets by a factor of 10!  I was able to reconstruct a full 2048x2048x1440 data set in
		< 5 minutes!  A lot still needs to be done to clean this up for general use if we decide to
		abandon HDF as an intermediary file format, but the performance advantage is huge!
1/28/2008	V2.3 BT	Modified code to not read a reconstruction.cfg file but to get that information
		from the command line.  This will allow for the program to be queued in a system like N1
		Grid Engine.  As queued applications can be reprioritized by the system, having a single
		reconstruction.cfg file is problematic as it's impossible to keep the file in sync with
		the correct application in the queue--now each instance can get that info from the
		command prompt.
1/28/2008	V2.3 BT Modified the location of the log files to be in the smaple tree.  Also allow
		for versioning of the log and override files.  This is done by creating a new log directory
		"logs_##" each time the sample is reconstructed.  This way, we can keep a running list
		of logs and configurations in case the sample is reconstructed multiple times.
1/30/2008  V2.3 BT Made it so the logs_ directory is prepended with 0 as needed so ls orders properly.
2/4/2008   V2.3 BT Added an error log file that will only be created if there is an error.  The
		error log will log all errros encountered by the system.  This provides a convenient location
		to look for problems if reconstructions fail or do not look quite right.
2/4/2008   V2.3 BT The system should no longer crash due to a raw data file missing.  Instead, the
		system will mark in the error log that the file is missing and will grab the previous file
		of the same type to fill in.  If a projection is missing, the previous projection will
		be used to complete the reconstruction.
2/11/2008  V2.3 BT CreateSinograms will now look to the file extension to decide if the file format
        is hdf or binary.  It expects .bin for binary and anything else defaults to HDF.
7/29/2009  V2.4 BT Added two new parameters to the cluster override_exp_file.config.  <start_fixed_shift
	   is the start value of a range of shifts.  <end_shift_value> is the end value of the range.
	   If <use_slices_file> is true, the slices in the slice file will be reconstructed at each of the
	   shifts in the range start-end and save in the reconstructed directory with a unique file name
	   containing the shift value.
6/30/2011 V2.5 Yongsheng Pan and BT Added the zero padding functionality to the GridRec algorithm. An entry named
          <GRIDREC Zero Padding> is added to the overwrite_exp_file.config, with GRIDREC_PADDING_NONE
          GRIDREC_PADDING_HALF(default) and GRIDREC_PADDING_ONE_AND_HALF as options. This functionality
          removes artifacts from GridRec.


__________________________________________________________________
__________________________________________________________________
NexusBox library

Top level interface to a group of classes used to read and write HDF files.
Developed and maintained by:
       Brian Tieman
       Argonne National Laboratory
       tieman@aps.anl.gov

8/20/2003  V4.0   BT  First version with acknowledgements
8/20/2003  V4.0   BT  Ported to Kylix
1/10/2006  V5.0   BT  Upgraded to make use of Nexus 3.0.0.  This allows for
		use of HDF5 to read/write files.  Modifications were made to handle
		data compression with the HDF5 data format.  HDF5 only supports LZW
		compression--thus, for HDF5, if RLE or HUF is requested, I made the
		to force LZW compression.  Also, the ability to write XML files is not
		tested very well--and isn't even built into the linux stuff.  The XML
		format is incredibly slow so it's unlikely anyone will want it anyway.









__________________________________________________________________
__________________________________________________________________
